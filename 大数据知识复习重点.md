

# 大数据知识复习重点

## Java编程基础

![](C:\article\Java框架复习重点.png)

## scala编程基础

Scala具有面向对象和面向函数编程的特性，具有python编程的灵活性，运行在JVM之上，也具有Java语音的规范性。（可以将Java、python、Scala语言对比着学习，融会贯通）

**编程语言学习策略：从简单入手，由易到难；先学习python版本的知识，快速上手；然后学习Java版本的代码，深挖技术细节和完善代码的健壮性**

**RDD编程实际就是python和Scala的lambda表达式以及高级函数使用**

## 大数据框架基础

个人准备的重点：hadoop，spark框架原理描述（流程图的大致描绘）；shuffle机制；调优的方向（数据倾斜、大量小文件处理问题）

**对分区以及shuffle的理解：**

Spark把rdd进行分区（分片），放在集群上并行计算。同一个rdd分片100个，10个节点，平均一个节点10个分区，当进行sum型的计算的时候，先进行每个分区的sum，然后把sum值shuffle传输到主程序进行全局sum，所以进行sum型计算对网络传输非常小。但对于进行join型的计算的时候，需要把数据本身进行shuffle，网络开销很大。

spark是如何优化这个问题的呢？

　　Spark把key－value rdd通过key的hashcode进行分区，而且保证相同的key存储在同一个节点上，这样对改rdd进行key聚合时，就不需要shuffle过程，我们进行mapreduce计算的时候为什么要进行shuffle？，就是说mapreduce里面网络传输主要在shuffle阶段，**shuffle的根本原因是相同的key存在不同的节点上，按key进行聚合的时候不得不进行shuffle**。shuffle是非常影响网络的，它要把所有的数据混在一起走网络，然后它才能把相同的key走到一起。要进行shuffle是存储决定的。

　　Spark从这个教训中得到启发，spark会把key进行分区，也就是key的hashcode进行分区，相同的key，hashcode肯定是一样的，所以它进行分区的时候100t的数据分成10分，每部分10个t，它能确保相同的key肯定在一个分区里面，而且它能保证存储的时候相同的key能够存在同一个节点上。比如一个rdd分成了100份，集群有10个节点，所以每个节点存10份，每一分称为每个分区，spark能保证相同的key存在同一个节点上，实际上相同的key存在同一个分区。

　　key的分布不均决定了有的分区大有的分区小。没法分区保证完全相等，但它会保证在一个接近的范围。所以mapreduce里面做的某些工作里边，spark就不需要shuffle了，spark解决网络传输这块的根本原理就是这个。

　　进行join的时候是两个表，不可能把两个表都分区好，通常情况下是把用的频繁的大表事先进行分区，小表进行关联它的时候小表进行shuffle过程。

　　大表不需要shuffle。

　　需要在工作节点间进行数据混洗的转换极大地受益于分区。这样的转换是 cogroup，groupWith，join，leftOuterJoin，rightOuterJoin，groupByKey，reduceByKey，combineByKey 和lookup。

![](C:\article\大数据框架及应用场景.png)

![](C:\article\大数据框架复习重点.png)

**数仓项目**

![](C:\article\数仓项目.png)

###### **推荐项目大致图示**



**关于笔试**
```
手写代码
➢手写 MapReduce的WordCount
➢手写Spark WordCount

手写设计模式
➢双端检测单例、工厂、代理、装饰模式

手写算法
➢详见算法部分

手写场景
➢详见场景部分
```
**关于算法**
```
数据结构必考，手写代码，每一面都会考。(今日头条)
➢用IDEA写快速排序
➢单向链表反转
➢快排的时间空间复杂度?快排原理➢ 手写二分查找
➢手写归并排序
➢字符串反转
➢二叉树的前中后序遍历?
➢冒泡的时间空间复杂度?原理
➢链表转置/二叉树转置
➢实现堆栈Push Pop Min复杂度0 (1)
```
**推荐**：

```
LeetCode
多刷Leetcode，题都是有套路的( 今日头条大数据）
➢两数之和
➢最长回文子串
➢爬楼梯
➢有效的括号
➢翻转二叉树
➢数组中的第K个最大元素
➢反转链表.
➢实现Trie (前缀树)
➢ LRU缓存机制
➢编辑距离
```
**关于场景**
```
➢写一个SQL将每个月的Top3取出来我用了三个子查询做出来不行
➢最近七天连续三天活跃用户怎么实现的?手写-一个各区域Top10商品统计程序?
➢三个字段，timestamp, user_ jid, product jid, 让求pv最大的商品，写了之后又叫
用Scala代码写- -遍， 然后又问，假如说只让你求pv数大于100的Top3呢，代码
又怎么写
➢有一个分区表，表名T,字段qq，age， 按天分区，让写出创建表的语句
```
**关于学习习惯部分**
```
看书
Hadoop专家、图解Spark、
Spark Streaming实时流式大数据处理实战、
基于Apache Flink的流处理、
Flink原理实战与性能优化等

看博客、写博客
CSDN、博客园、简书等

看官网、GitHub
Apache官网/GitHub

研究新技术
➢Flink、 Atlas、 Griffin、 Kylin、 ClickHouse等
```
**关于提问面试官**
````
面试官:您还有什么想问我的吗?
➢这是体现个人眼界和层次的问题
➢参考答案
V公司希望我入职后的3-6个月内，给公司解决什么样的问题
V以你现在对我的了解，您觉得我需要多长时间融入公司?
```
## 前沿技术了解

![](C:\article\前沿技术.png)

## 项目相关

### Spark MLlib数据挖掘

数据预处理过程（数据清洗、数据归一化、降维度）

以下算法的实现以及与单机版算法的区别：

- 支持向量机SVM

- 随机森林

- 神经网络

- LSTM

  ![](C:\article\时间序列LSTM建模流程图.png)

针对数据降噪，针对因子数据降维

### 深度学习+Spark

Keras/Elephas

http://maxpumperla.com/elephas/#distributed-hyper-parameter-optimization

```python
#首先，创建本地pyspark上下文
from pyspark import SparkContext, SparkConf
conf = SparkConf().setAppName('Elephas_App').setMaster('local[8]')
sc = SparkContext(conf=conf)

接下来，您定义并编译Keras模型
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.optimizers import SGD
model = Sequential()
model.add(Dense(128, input_dim=784))
model.add(Activation('relu'))
model.add(Dropout(0.2))
model.add(Dense(128))
model.add(Activation('relu'))
model.add(Dropout(0.2))
model.add(Dense(10))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer=SGD())

并从numpy数组创建一个RDD（或者您想创建一个RDD）
from elephas.utils.rdd_utils import to_simple_rdd
rdd = to_simple_rdd(sc, x_train, y_train)

# Elephas中的基本模型是SparkModel。SparkModel通过传入已编译的Keras模型，更新频率和并行化模式来初始化a 。之后，您可以简单地fit在RDD上建立模型。亚洲象fit 具有相同的选项作为Keras模式，这样你就可以通过epochs，batch_size等，你从Keras使用。
from elephas.spark_model import SparkModel
spark_model = SparkModel(model, frequency='epoch', mode='asynchronous')
spark_model.fit(rdd, epochs=20, batch_size=32, verbose=0, validation_split=0.1)

#使用spark-submit运行脚本
#可能需要进一步增加驱动程序内存，因为网络中的参数集可能非常大，并且在驱动程序上收集它们会消耗大量资源。有关示例，请参见examples文件夹。
spark-submit --driver-memory 1G ./your_script.py
```



### spark streaming 实时处理项目

慕课网课程访问量实时分析项目

http://dblab.xmu.edu.cn/post/8274/

https://blog.csdn.net/zfszhangyuan/article/details/52522974

https://blog.csdn.net/qq_41851454/article/details/80402483

Realtime prediction using Spark Structured Streaming, XGBoost and Scala

https://towardsdatascience.com/realtime-prediction-using-spark-structured-streaming-xgboost-and-scala-d4869a9a4c66

### 流线性回归（Streaming linear regression）

当数据以流方式到达时，在线拟合回归模型，在新数据到达时更新模型的参数非常有用。`spark.mllib`当前支持使用普通最小二乘法进行流线性回归。拟合与脱机执行的拟合相似，不同之处在于拟合发生在每批数据上，因此模型会不断更新以反映流中的数据。

We can now save text files with data to the training or testing folders. Each line should be a data point formatted as `(y,[x1,x2,x3])` where `y` is the label and `x1,x2,x3` are the features. Anytime a text file is placed in `sys.argv[1]` the model will update. Anytime a text file is placed in `sys.argv[2]` you will see predictions. As you feed more data to the training directory, the predictions will get better!

python版本代码

```python
mport sys

from pyspark.mllib.linalg import Vectors
from pyspark.mllib.regression import LabeledPoint
from pyspark.mllib.regression import StreamingLinearRegressionWithSGD

def parse(lp):
    label = float(lp[lp.find('(') + 1: lp.find(',')])
    vec = Vectors.dense(lp[lp.find('[') + 1: lp.find(']')].split(','))
    return LabeledPoint(label, vec)

trainingData = ssc.textFileStream(sys.argv[1]).map(parse).cache()
testData = ssc.textFileStream(sys.argv[2]).map(parse)

numFeatures = 3
model = StreamingLinearRegressionWithSGD()
model.setInitialWeights([0.0, 0.0, 0.0])

model.trainOn(trainingData)
print(model.predictOnValues(testData.map(lambda lp: (lp.label, lp.features))))

ssc.start()
ssc.awaitTermination()
```
Scala版本代码
```scala
Scalaimport org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.regression.StreamingLinearRegressionWithSGD

val trainingData = ssc.textFileStream(args(0)).map(LabeledPoint.parse).cache()
val testData = ssc.textFileStream(args(1)).map(LabeledPoint.parse)

val numFeatures = 3
val model = new StreamingLinearRegressionWithSGD()
  .setInitialWeights(Vectors.zeros(numFeatures))

model.trainOn(trainingData)
model.predictOnValues(testData.map(lp => (lp.label, lp.features))).print()

ssc.start()
ssc.awaitTermination()
```

## 流式均值

当数据到达流中时，我们可能希望动态估计群集，并在新数据到达时对其进行更新。`spark.mllib`为流式k均值聚类提供支持，并提供参数以控制估计值的衰减（或“健忘”）。该算法使用了小批量k均值更新规则的概括。对于每一批数据，我们将所有点分配给它们最近的聚类，计算新的聚类中心，然后使用以下方法更新每个聚类：

``

```
ct+1=ctntα+xtmtntα+mt(1)(1)ct+1=ctntα+xtmtntα+mt
nt+1=nt+mt(2)(2)nt+1=nt+mt
```



其中，是群集的上一个中心，是到目前为止分配给群集的点数，是当前批次中新的群集中心，是当前批次中 添加到群集中的点数。衰减因子 可用于忽略过去：所有数据将从一开始就使用；与只有最近的数据将被使用。这类似于指数加权移动平均值。`ctct``ntnt``xtxt``mtmt``αα``αα=1``αα=0`

可以使用一个`halfLife`参数指定衰减，该参数确定正确的衰减因子，`a`这样对于在时间获取的数据`t`，其按时间的贡献`t + halfLife`将降至0.5。可以将时间单位指定为`batches`或`points`，更新规则将相应地进行调整。

```python
from pyspark.mllib.linalg import Vectors
from pyspark.mllib.regression import LabeledPoint
from pyspark.mllib.clustering import StreamingKMeans

# we make an input stream of vectors for training,
# as well as a stream of vectors for testing
def parse(lp):
    label = float(lp[lp.find('(') + 1: lp.find(')')])
    vec = Vectors.dense(lp[lp.find('[') + 1: lp.find(']')].split(','))

    return LabeledPoint(label, vec)

trainingData = sc.textFile("data/mllib/kmeans_data.txt")\
    .map(lambda line: Vectors.dense([float(x) for x in line.strip().split(' ')]))

testingData = sc.textFile("data/mllib/streaming_kmeans_data_test.txt").map(parse)

trainingQueue = [trainingData]
testingQueue = [testingData]

trainingStream = ssc.queueStream(trainingQueue)
testingStream = ssc.queueStream(testingQueue)

# We create a model with random clusters and specify the number of clusters to find
model = StreamingKMeans(k=2, decayFactor=1.0).setRandomCenters(3, 1.0, 0)

# Now register the streams for training and testing and start the job,
# printing the predicted cluster assignments on new data points as they arrive.
model.trainOn(trainingStream)

result = model.predictOnValues(testingStream.map(lambda lp: (lp.label, lp.features)))
result.pprint()

ssc.start()
ssc.stop(stopSparkContext=True, stopGraceFully=True)
```

### Combining Spark Streaming + MLlib

Yes, you can use model generated from static data. The problem you experience is not related to streaming at all. You simply cannot use JVM based model inside action or transformations (see [How to use Java/Scala function from an action or a transformation?](https://stackoverflow.com/q/31684842/1560062) for an explanation why). Instead you should apply `predict` method to a complete `RDD` for example using `transform` on `DStream`:

```python
from pyspark.mllib.tree import RandomForest
from pyspark.mllib.util import MLUtils
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
from operator import attrgetter


sc = SparkContext("local[2]", "foo")
ssc = StreamingContext(sc, 1)

data = MLUtils.loadLibSVMFile(sc, 'data/mllib/sample_libsvm_data.txt')
trainingData, testData = data.randomSplit([0.7, 0.3])

model = RandomForest.trainClassifier(
    trainingData, numClasses=2, nmTrees=3
)

(ssc
    .queueStream([testData])
    # Extract features
    .map(attrgetter("features"))
    # Predict 
    .transform(lambda _, rdd: model.predict(rdd))
    .pprint())

ssc.start()
ssc.awaitTerminationOrTimeout(10)
```

